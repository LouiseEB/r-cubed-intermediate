# Processing datasets for cleaning {#dplyr-1}

![](https://img.shields.io/badge/document%20status-rough%20draft-red?style=flat-square)

```{r, eval=TRUE, child="includes/preamble.Rmd"}
```

```{r setup-04, include=FALSE, eval=FALSE}
source("R/functions.R")
library(here)
library(fs)
library(dplyr)
library(stringr)
library(purrr)
library(vroom)
source_session("03-dry.Rmd")
knitr::opts_chunk$set(eval = FALSE)
```

Here we will continue making use of the "*Workflow*" block as we cover the third
block, "*Create final data*" in Figure \@ref(fig:diagram-overview-3).

```{r diagram-overview-3, fig.cap="Section of the overall workflow we will be covering.", echo=FALSE}
diagram_overview(3)
```

And your folder and file structure should look like:

```
LearnR3
├── data/
│   └── README.md
├── data-raw/
│   ├── mmash-data.zip
│   ├── mmash/
│   │  ├── user_1
│   │  ├── ...
│   │  └── user_22
│   └── mmash.R
├── doc/
│   ├── README.md
│   └── lesson.Rmd
├── R/
│   ├── functions.R
│   └── README.md
├── .Rbuildignore
├── .gitignore
├── DESCRIPTION
├── LearnR3.Rproj
└── README.md
```

## Converting between wide and long data

We've covered the basic "verbs" (functions) of the dplyr package.
Now we'll get into the "pivot" functions from dplyr's companion package, tidyr. 
There are many useful functions in the tidyr package, 
but the pivot functions (`pivot_longer()` and `pivot_wider()`) are key ones.
Pivoting converts wide data into long data or vice versa.
So what is wide or long date?

Wide data is data where values may repeat across columns. 
With repeated measurements, it is often easier to enter data in wide form
or to use wide data to present in tables.
But there are problems with wide data, especially when it comes to analysing it. 
For instance, wide data may look like:

```{r table-example-wide, echo=FALSE}
example_wide <- tribble(
    ~PersonID, ~Glucose_0, ~Glucose_30, ~Glucose_60,
    1, 5.6, 7.8, 4.5,
    2, 4.7, 9.5, 5.3,
    3, 5.1, 10.2, 4.2
) 

example_wide %>% 
    knitr::kable(caption = "Example of a **wide** dataset that is useful for data entry.",
                 align = "c")
```

However, this type of data is not *tidy* for a few reasons:

1. We don't know precisely what the values represent in the glucose columns
(though in this case we could guess, but this isn't always the case).
Often in the wide form you need to rely a lot more on either very descriptive names
or have a detailed data dictionary to refer to.
1. The glucose columns all represent the same value type (glucose concentration) 
so there is some duplication *of meaning* between columns.
1. The column names include data in them as well (time of glucose measurement). 

On the other hand, a long data form is usually better suited for almost any type of analysis
and for visualizing, especially when doing [split-apply-combine] techniques. 
Long form data also tends to be more *tidy* compared to wide form.

```{r table-example-long, echo=FALSE}
example_wide %>% 
    pivot_longer(-PersonID, names_to = "MeasurementTime", values_to = "GlucoseConcentration", 
                 names_prefix = "Glucose_") %>% 
    mutate(MeasurementTime = as.numeric(MeasurementTime)) %>% 
    knitr::kable(caption = "Example of a **long** dataset that is more usable for analyses and visualizing.",
                 align = "c")
```

### Pivot from wide to long

How, when, and why to pivot your data can be conceptually challenging to grasp at first. 
Let's try out some examples and use `pivot_longer()`. 
Like all the other functions, the first position argument to `pivot_longer()` is the data.
The other necessary arguments (in order) are:

1. `cols`: The columns to use to convert to long form. 
The input is a vector made using `c()` that contains the column names,
like you would use in `select()` (e.g. you can use the `select_helpers` like `starts_with()`,
or `-` minus to exclude).
1. `names_to`: The name of the newly created column (as a quoted character) 
that contains the original column names.
1. `values_to`: The name of the newly created column (as a quoted character)
that contains the original cells of the original columns.

As with everything, using an example would help clarify things.
In the NHANES dataset, there are several columns that would be suitable for pivoting,
because they are "messy". These are the `BP` blood pressure columns.
Let's select the required `ID` and `SurveyYr` and the `BP` columns
(we'll exclude the `Ave` ones for now), then we'll use `pivot_longer()`.
Because we only want to pivot the `BP`, we need to exclude `ID` and `SurveyYr`
from pivoting by using `-`.

```{r}
NHANES %>%
    # Recall that - (minus) is used to exclude
    select(ID, SurveyYr, starts_with("BP"), -ends_with("Ave")) %>% 
    pivot_longer(c(-ID, -SurveyYr), names_to = "BPTypeAndNumber", values_to = "BloodPressure")
```

We use `-` here to tell `pivot_longer()` to *not* include 
(to *exclude*) the columns from being converted to long form.
We could even use `starts_with()`:

```{r}
NHANES %>%
    # Recall that - (minus) is used to exclude
    select(ID, SurveyYr, starts_with("BP"), -ends_with("Ave")) %>% 
    pivot_longer(starts_with("BP"), names_to = "BPTypeAndNumber", values_to = "BloodPressure")
```

The reason that the arguments `names_to` 
and `values_to` require quoting `""` is that these are the column names that we *will* create.
Because they don't exist yet as columns, we need to use the quotes.

### Pivot from long to wide

You can also convert from long to wide, though this is less commonly done,
as most analyses either work better or require the long form. 
It can also be much more tricky to convert over to.
But sometimes you may need to have a wide form for your data.
Here you can use `pivot_wider()` function. Others have used `pivot_wider()` for creating summary tables for presentation or to capture data in a format needed by other tools. Like its opposite,
the first position argument is the data and the other necessary arguments are:

1. `id_cols`: This is optional as it will default to all column names. 
This argument tells `pivot_wider()` to use the given columns as the identifiers
for when converting. 
This is where the tricky part comes in, 
because 
1. `names_from`: Similar to the `pivot_longer()`, 
this is the name of the column that will make up the new columns.
Unlike in `pivot_longer()`, 
the column name given is *unquoted* since the column 
*must already exist* in the dataset.
1. `values_from`: As with above, this is the column name 
(that exists and must be given *unquoted*) 
for the values that will be in the new columns.

Unfortunately, 
NHANES as it is doesn't have a structure that allows us to easily convert to wide form.
So we'll use the `table2` example from the beginning of the wrangling section:

```{r}
table2
```

This data frame is in a very long format. 
So we could pivot `type` and `count` into the wide format:

```{r}
table2 %>% 
    pivot_wider(names_from = type, values_from = count)
```

If we wanted to make it even wider, 
we could include `year` and `type` in the pivoting by wrapping them with `c()`
in the `names_from` argument:

```{r}
table2 %>% 
    pivot_wider(names_from = c(year, type), values_from = count)
```

The key to using `pivot_wider()` is that there are uniquely identifying rows
that allow pivoting that maintains the integrity of the data.
Since the NHANES dataset we use is for teaching purposes,
there are some cases where the same person is recorded multiple times in one survey year,
which doesn't make sense and prevents us from adequately pivoting wider.

## Pivot, then split-apply-combine {#pivot-sac}

The real strength of pivoting is when you use it with the [split-apply-combine] method.
For instance, if we wanted to find some simple statistics of all the columns
by survey year and sex. 
In this case, we would use `pivot_longer()` to convert to a long form
and then use `group_by()` and `summarize()` to find the simple statistics.
So let's find the mean values of some continuous variables.

```{r}
nhanes_mean_values <- NHANES %>%
    rename(Sex = Gender) %>%
    select(SurveyYr, Sex, BMI, Age, starts_with("BP")) %>%
    pivot_longer(c(-SurveyYr, -Sex),
                 names_to = "Variables",
                 values_to = "Values") %>%
    group_by(SurveyYr, Sex, Variables) %>% 
    summarize(MeanValues = mean(Values, na.rm = TRUE))
nhanes_mean_values
```

We could now use `pivot_wider()` since the structure allows for it:

```{r}
nhanes_mean_values %>% 
    pivot_wider(names_from = Variables, values_from = MeanValues)
```

Which now gives us the mean values of the variables by sex and survey year!

## Processing character data

**Take 8 min to read this section before we quickly go over it together**.
When processing data, you likely will encounter and deal with cleaning up
character data. A wonderful package to use for working with character data is
called [stringr]. We'll use that in order to process the `file_path_id` 
so that we can get the user ID from it. First, let's go to the `setup`
code chunk and replace purrr with tidyverse and move `library(tidyverse)` to
the top of the `setup` code chunk.

The main driver behind the functions in stringr are [regular expressions] (or
[regex] for short). These expressions are powerful, very concise ways of finding
patterns in text. We've already used them in the `dir_ls()` function with the
`regexp` argument to find our data files.

To give an example, the regex `^.*[1-9][0-9]?` means "starting from the
beginning of the text, find any characters one or more times and stop at a
number from 0 to 9, include a possible other number from 0 to 9". So, using this
on a string like `"hi there, it's 30 degrees out"`, the regex would select `"hi
there, it's 30"`. If we break it down:

- `^` means start of the string.
- `.` means match any character once.
- `*` means match the previous character one or more times.
- `[]` means match whatever is in this brack once.
- `0-9` means match any number from 0 to 9.
- `?` means the previous thing may also be matched a second time.

Confused? Yea, regex does that to almost everyone, you aren't alone. While
regex can be very very powerful, it can also be incredibly difficult to write
and work with. We won't cover this anymore in this course, but two great resources
are the [R for Data Science regex section](https://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions),
the [stringr regex page](https://stringr.tidyverse.org/articles/regular-expressions.html),
as well as in the help doc `?regex`.
For now, we will only use it for the simplest purposes possible:
to extract `user_1` to `user_22` from the `file_path_id`.
**Please stop here and we'll go over this together**.

[stringr]: https://stringr.tidyverse.org/

`r details_for_instructors("
Make sure to re-inforce that while regex is incredibly complicated, there are some
basic things you can do with it that are quite powerful.

More or less, this section is to introduce the idea and concept of regex, but not
to really teach it since that is well beyond the scope of this course and this
time frame.
")`

## Exercise: Brainstorm a regex that will match for the user ID

Time: 5 min

Discuss with your neighbour what a potential regex might be to find
the ID in the `file_path_id`. Try not to look ahead `r emo::ji("wink")` since we
will use this regex later on. When the time is up, we'll share possible ideas.

## Exercise: What is the `%>%` pipe?

Time: 5 min

`r details_for_instructors("
Before starting this exercise, ask how many have used the pipe before.
If everyone has, then move on to the next section.
")`

We haven't used the `%>%` pipe from the [magrittr] package yet, but it is used
extensively in many R package, is the foundation of tidyverse packages,
and will eventually be incorporated into the next version of R (rather than
through magrittr). Because of this, we will make heavy use of it. To make sure
everyone is on the same page please do either:

[magrittr]: https://magrittr.tidyverse.org/

- If one of you or your pair doesn't know what the pipe is, take some time to 
talk about and explain it (if you know).
- If neither of the pair knows, please read 
[the section on it](https://r-cubed.rostools.org/wrangling.html#chaining-functions-with-the-pipe)
from the beginner course.

## Working with character columns

Now that we've talked about regex and pipes, let's start using them.
The first thing we'll do is work with the `user_info_df` to write
code that works, after which we will convert it into a function and move it
into the `R/functions.R` file.

We want to create a new column for the user ID, so we will use the `mutate()`
function from the [dplyr] package. We'll use the regex `user_[0-9][0-9]?` to match
for the user ID and we'll use the `str_extract()` function from the stringr
package. So, in your `doc/lesson.Rmd` file, create a new header called 
`## Using regex for user ID` at the bottom of the document, and create a new
code chunk below that.

[dplyr]: https://dplyr.tidyverse.org/

`r details_for_instructors("
Walk through writing this code, explain about how to use mutate, and about
the stringr function.
")`

```{r extract-user-id, eval=FALSE}
# Note: your file paths and data may look slightly different.
user_info_df %>% 
    mutate(user_id = str_extract(file_path_id, "user_[0-9][0-9]?"))
```

```{r extract-user-id-eval, echo=FALSE}
user_info_df %>% 
    mutate(file_path_id = gsub(".*\\/data-raw", "data-raw", file_path_id)) %>% 
    mutate(user_id = str_extract(file_path_id, "user_[0-9][0-9]?")) %>% 
    relocate(file_path_id, user_id)
```

Since we don't need to keep the `file_path_id`, let's drop it using `select()`
and `-`.

```{r drop-file-path-id}
user_info_df %>% 
    mutate(user_id = str_extract(file_path_id, "user_[0-9][0-9]?")) %>% 
    select(-file_path_id)
```

## Exercise: Convert this code into a function

Time: 10 min

We now have code that takes the data that has the `file_path_id` column
and extracts the user ID from it. While in the `doc/lesson.Rmd` file, convert
this code into a function, using the same process you've done previously.

- Call the new function `extract_user_id` and add one argument called
`imported_data`.
    - Don't forget to output the code into an object and adding `return()` at
    the end with the object inside it. 
    - Also include Roxygen documentation.
- After writing it and testing it, move the function into `R/functions.R`.
- Replace the code in the `doc/lesson.Rmd` file with the function name (e.g.
`extract_user_id(user_info_df)`), load everything with `load_all()`
(`Ctrl-Shift-L`), and run the new function.

*Tip*: If you don't know what package a function comes from when you
need to append the package when using `::`, you can find out what the package is
by using the help documentation `?functionname` (can also be done by pressing F1
when the cursor is over the function). The package name is at the very top left
corner, surrounded by `{ }`.

```{r solution-user-id-extract, echo=FALSE}
extract_user_id <- function(imported_data) {
    extracted_id <- imported_data %>% 
        dplyr::mutate(user_id = stringr::str_extract(file_path_id, 
                                                     "user_[0-9][0-9]?")) %>% 
        dplyr::select(-file_path_id)
    return(extracted_id)
}
```

Since we want this function to work on all data that we import, we should add it
to `import_multiple_files()`. After you've created the function, go to the
`import_multiple_files()` function in `R/functions.R` and use the `%>%` to
add it after using the `map_dfr()` function. The code should look something like:

```{r add-extract-user-to-import}
import_multiple_files <- function(file_pattern, import_function) {
    data_files <- fs::dir_ls(here::here("data-raw/mmash/"),
                             regexp = file_pattern,
                             recurse = TRUE)
    
    combined_data <- purrr::map_dfr(data_files, import_function,
                                    .id = "file_path_id") %>% 
        extract_user_id()
    return(combined_data)
}
```

Re-load the functions with `load_all()` (`Ctrl-Shift-L`). Then re-run these
pieces of code you wrote during Exercise \@ref(ex-function-import-all-data) to
update them based on the new code in the `import_multiple_files()` function.

```{r}
user_info_df <- import_multiple_files("user_info.csv", import_user_info)
saliva_df <- import_multiple_files("saliva.csv", import_saliva)
rr_df <- import_multiple_files("RR.csv", import_rr)
actigraph_df <- import_multiple_files("Actigraph.csv", import_actigraph)
```

## Join datasets together

The ability to join datasets together is a fundamental component of data processing
and transformation. In our case, we want to add the datasets together so we 
eventually have at least one main dataset to work with.

There are many ways to join datasets, the more common ones that are implemented
in the dplyr package are:
in the dplyr package are:

- `left_join(x, y)`: Join all rows and columns in `y` that match rows and
columns in `x`. *Columns* that exist in `y` but not `x` are joined to `x`.

    ```{r image-left-join, fig.cap="Left joining in dplyr. Modified from the [RStudio dplyr cheatsheet][dplyr-cheatsheet].", out.width="60%", echo=FALSE}
    knitr::include_graphics(here::here("images/left-join.svg"))
    ```
    
- `right_join(x, y)`: The opposite of `left_join()`. Join all rows and columns
in `x` that match rows and columns in `y`. *Columns* that exist in `x` but not `y`
are joined to `y`.

    ```{r image-right-join, fig.cap="Right joining in dplyr. Modified from the [RStudio dplyr cheatsheet][dplyr-cheatsheet].", out.width="60%", echo=FALSE}
    knitr::include_graphics(here::here("images/right-join.svg"))
    ```

- `full_join(x, y)`: Join all rows and columns in `y` that match rows and
columns in `x`. Columns *and* **rows** that exist in `y` but not `x` are joined
to `x`. This is probably the more commonly used one, as any missing values that
show up are probably important to look into.

    ```{r image-full-join, fig.cap="Full joining in dplyr. Modified from the [RStudio dplyr cheatsheet][dplyr-cheatsheet].", out.width="60%", echo=FALSE}
    knitr::include_graphics(here::here("images/full-join.svg"))
    ```

[dplyr-cheatsheet]: https://raw.githubusercontent.com/rstudio/cheatsheets/master/data-transformation.pdf

In our case, we want to use `full_join()`, since we want all the data from both
datasets. This function takes two datasets and lets you indicate which column
to join by using the `by` argument. Here, both datasets have the column `user_id`
so we will join by them.

```{r}
full_join(user_info_df, saliva_df, by = "user_id")
```

We also eventually have other datasets to join together later on. Since
`full_join()` can only take two datasets at a time, do we then just keep 
using `full_join()` until all the other datasets are combined?
What if we get more data later on? Well, that's where more functional programming
comes in. Again, we have a simple goal: For a set of data frames, join them
all together. Here we use another functional programming concept called `reduce()`.
Like `map()`, which "maps" a function onto a set of items, `reduce()`
applies a function to each item of a vector or list, each time reducing the set
of items down until only one remains: the output. Let's use our simple function
`add_numbers()` from before and add up 1 to 5. Since `add_numbers()` only takes
two numbers, we have to give it two numbers at a time and repeat until we reach
5.

```{r add-numbers-function, echo=FALSE}
add_numbers <- function(num1, num2) {
    added <- num1 + num2
    return(added)
}
```

```{r}
# Add from 1 to 5
first <- add_numbers(1, 2)
second <- add_numbers(first, 3)
third <- add_numbers(second, 4)
add_numbers(third, 5)
```

Instead, we can use reduce to do the same thing:

```{r}
reduce(1:5, add_numbers)
```

This Figure \@ref(fig:image-reduce) visually shows what is happening within `reduce()`.

```{r image-reduce, fig.cap="A functional that iteratively uses a function on a set of items until only one output remains. Modified from the [RStudio purrr cheatsheet][purrr-cheatsheet].", out.width="75%", echo=FALSE}
knitr::include_graphics(here::here("images/reduce.svg"))
```

[purrr-cheatsheet]: https://raw.githubusercontent.com/rstudio/cheatsheets/master/purrr.pdf

So, for our `full_join()`, if we put the datasets together as a list, we can
`reduce()` them down into one dataset with `full_join()`.

```{r}
combined_data <- reduce(list(user_info_df, saliva_df), full_join)
combined_data
```

## Renaming all columns

TODO: for saliva data, need to include a day column based on samples

Functionals appear throughout R and are especially used frequently in the
tidyverse packages like dplyr. In order to continue processing the other datasets,
we need to get into these other functions.

```{r}
combined_data %>% 
    rename_with(snakecase::to_snake_case)
```


```{r}
import_rr <- function(file_path) {
    rr_data <- vroom::vroom(
        file_path,
        col_select = -1,
        col_types = vroom::cols(
            ibi_s = vroom::col_double(),
            day = vroom::col_double(),
            # Converts to seconds
            time = vroom::col_time(format = "")
        ),
        .name_repair = snakecase::to_snake_case
    ) 
    return(rr_data)
}
```

summarize across is also a functional in that you can give it many functions
to use on each of the columns you want the function to work on.

The across functional can also take a function when selecting the columns,
just like with the `select()` function
e.g. if you want to select only numeric columns you would use:

```{r}
user_info_df %>% 
    select(where(is.numeric))
```

Like wise with summarize, you can use across the same way:

```{r}
user_info_df %>% 
    summarize(across(where(is.numeric), mean))

user_info_df %>% 
    summarize(across(where(is.numeric), list(mean = mean, sd = sd)))
```

## Summarizing larger data to join

We use this concept to process the other longer datasets like `RR.csv` so we can
join them with the `user_info.csv` and other smaller datasets.

```{r}
import_multiple_files <- function(file_pattern, import_function) {
    data_files <- fs::dir_ls(here::here("data-raw/mmash/"),
                             regexp = file_pattern,
                             recurse = TRUE)
    
    combined_data <- purrr::map_dfr(data_files, import_function,
                                    .id = "file_path_id") %>% 
        extract_user_id()
    return(combined_data)
}
library(tidyverse)

rr_df %>% 
    group_by(user_id, day) %>% 
    summarize(across(ibi_s, list(mean = mean, sd = sd)))
```

We get a message:

```text
`summarise()` regrouping output by 'user_id' (override with `.groups` argument)
```

If we look into `?summarize` to the `.groups` argument,
we see that this argument is currently experimental. At the bottom there is a 
message about:

> In addition, a message informs you of that choice, unless the option
"dplyr.summarise.inform" is set to FALSE, or when summarise() is called from a
function in a package.

How would we do that? By using the `options()` function. So, go to the `setup`
code chunk and add this code to the top:

TODO: Make this true to show what it puts, since this is already in _common.R
```{r}
options(dplyr.summarize.inform = FALSE)
```



## Exercise: Do same with Actigraph

- Look into documentation.
    - Rename columns to be more descriptive (e.g. what does `hr` mean? or `axis1`)
    by using the `rename()` from dplyr. Use the help `?rename` to figure out how 
    to use it if you don't know yet.
- Based on the documentation, which variables would you be most interested in
analyzing more?
    - Keep only those columns by using `select()`.
    - Decide which summary measure you think may be most interesting for you
    (e.g. `median()`, `sd()`, `mean()`, `max()`, `min()`, `var()`)
- Using `group_by()` of `user_id` and `day`, summarize the variables you chose

- group_by and summarize (FP)
- selecting

## Exercise: activity, calculate minutes from start and end.

Time: 25 minutes

- Create a new header called `## Exercise: Importing activitiy data`
- Create a new code chunk
- Starting from the beginning (e.g. using the `spec()` process) and write code
that imports the `Activity.csv` file. 
- Convert this code into a new function using the workflow you've used from this
course.
    - Called `import_activity`
    - Have one argument called `file_path`, import with `import_multiple_files()`.
    - Test that it works
    - Move
- Create a new column called `activity_minutes` by 

- Look into the data documentation and figure out what the columns mean.
- Based on the 

```{r}
import_activity <- function(file_path) {
    
}
```



## Exercise

1. Look at the [data dictionary][mmash-site] and find out what the columns
mean and rename the column to be something more meaningful and
readable. 
TODO: Include this exercise? 
    - Pipe `%>%` the output from `vroom()` into the `rename()` function from the
    dplyr package to rename the column and use [`snake_case`][snake-case] when
    naming the new column. Read the `?dplyr::rename` help to know how to rename
    columns.

1. Read through the Data Description and rename the columns to be more readable
and explicit.
TODO: Change this last one?

## Wrangling data into final form

Save in data-raw script, source, and save data in data/

## Exercise: What other cleaned data might you create?

Time: 10 min

In order to meaningfully join all the datasets together, we have to calculate
summary measures for the `RR.csv` and `Actigraph.csv` data. However,
we also lose a lot of information and potentially interesting analyses from it.
With your neighbour, discuss some potential ways that you might join the datasets
to retain that information and what questions you might be interested in with
that data.

## Easily add parallel processing

**Take 8 minutes to read through this section and then move on to the last exercise**.
One major reason to get comfortable with and good at using purrr functions like 
`map()` is because it is relatively trivial to use parallel processing to speed
up your analysis. Packages like [furrr] (a combination of the [future] and [purrr] package)
have a series of functions starting with `future_` that can convert code using
`map()` into parallel processing code by switch to `future_map()`.
This extends to also `future_map_chr()`, `future_map_dfr()`, and so on.
What this does is creates multiple R sessions that run code simultaneously
and then eventually merges the results back into one R session.
But *note*, that parallel processing is good for some things and not for others.
If you code requires a specific sequence to run (1 relies on 2 which relies on 3),
parallel processing is not the right tool. It also doesn't always speed up tasks.
Creating the multiple R sessions and then merging them back takes some time,
so if your tasks is already pretty fast, using parallel processing might actually
make things slower.

furrr works by using the function `plan()` and setting a processing strategy.
There are really only two right now: `sequential` that you already use
and `multisession` that runs the parallel processing.
*Note*, the `plan()` function should **not** be put into a function. 
It should be included in an R script (like `data-raw/mmash.R`)
on its own. Here's an example of a 7 line R script:

[furrr]: https://davisvaughan.github.io/furrr/index.html
[future]: https://cran.r-project.org/package=future

```{r parallel-processing, eval=FALSE}
# Add this to the top:
library(furrr)
plan(multisession)

future_map(1:5, paste)

# Add this to end so it returns to normal
plan(sequential)
```

The ending should generally have `plan(sequential)` so you tell R to switch back 
to normal.

## Exercise: Add parallel processing to your raw data processing

Time: 10 min

Let's compare the difference between not using parallel processing and using it.
First:

1. Open the `data-raw/mmash.R` file.
1. Restart the R session `Ctrl-Shift-F10`.
1. Re-run the script with `source()` by using either the button "Source" at the
top right corner of the RStudio pane or with `Ctrl-Shift-S`.

After it finishes, then:

1. Add `library(furrr)` with the other `library()` functions at the top of 
`data-raw/mmash.R`.
1. Add `plan(multisession)` right below all the other library functions.
1. Go to the `import_multiple_df()` function in `R/functions.R` and replace
`purrr::map_dfr()` with `furrr::future_map_dfr`.
1. Go back to the `data-raw/mmash.R` script and add `plan(sequential)` to the very end.
1. Restart the R session with `Ctrl-Shift-F10`.
1. Re-run the code with `source()` using either the button "Source" at the top
right corner of the RStudio pane or with `Ctrl-Shift-S`. 

Do you notice a difference in speed compared to before?
