# Processing and joining datasets for cleaning {#dplyr-joins}

```{r setup-05, include=FALSE, eval=FALSE}
source("R/functions.R")
library(here)
library(fs)
library(tidyverse)
library(vroom)
source_session("03-functions.Rmd")
source_session("04-functionals.Rmd")
```

Here we will continue using the "*Workflow*" block and start moving over to the
third block, "*Create final data*", in Figure \@ref(fig:diagram-overview-4).

```{r diagram-overview-4, fig.cap="Section of the overall workflow we will be covering.", echo=FALSE}
knitr::include_graphics("images/diagram-overview-3.svg")
```

And your folder and file structure should look like:

```
LearnR3
├── data/
│   ├── mmash.rda
│   └── README.md
├── data-raw/
│   ├── mmash-data.zip
│   ├── mmash/
│   │  ├── user_1
│   │  ├── ...
│   │  └── user_22
│   └── mmash.R
├── doc/
│   ├── README.md
│   └── lesson.Rmd
├── R/
│   ├── functions.R
│   └── README.md
├── .gitignore
├── DESCRIPTION
├── LearnR3.Rproj
└── README.md
```

## Learning objectives

1. Learn what regular expressions are and how to use them on character data.
1. Learn about and apply the various ways data can be joined.
1. Apply functionals when repeatedly joining multiple datasets.
1. Apply the function `case_when()` when you need nested conditionals for cleaning.
1. Use the `usethis::use_data()` function to save the final, fully joined
dataset as an `.Rda` file in `data/`.

## Processing character data

**Take 5 min to read this section before continuing on to the exercise**.
Before we go into joining datasets together, we have to do a bit of processing
first. Specifically, we want to get the user ID from the `file_path_id`
character data. Whenever you are processing and cleaning data, you will very
likely encounter and deal with character data. A wonderful package to use for
working with character data is called [stringr], which we'll use to extract the
user ID from the `file_path_id` column.

[stringr]: https://stringr.tidyverse.org/

The main driver behind the functions in stringr are *regular expressions* (or
*regex* for short). These expressions are powerful, very concise ways of finding
patterns in text. Because they are so concise, though, they are also *very very
difficult* to learn, write, and read, even for experienced users. That's because
certain characters like `[` or `?` have special meanings. For instance,
`[aeiou]` is regex for "find one character in a string that is either a, e, i, o, or u".
The `[]` in this case mean "find the character in between the two brackets".
We won't cover regex too much in this course, some great resources for learning
them are the
[R for Data Science regex section](https://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions),
the [stringr regex page](https://stringr.tidyverse.org/articles/regular-expressions.html),
as well as in the help doc `?regex`.

We've already used them a bit in the `dir_ls()` function with the `regexp`
argument to find our data files. In the case of the regex in our use of
`dir_ls()`, we had wanted to find, for instance, the pattern `"user_info.csv"`
in all the folders and files. 
But in this case, we want to extract the user ID pattern, from `user_1` to `user_22`.
So how would we go about extracting this pattern?

## Exercise: Brainstorm a regex that will match for the user ID

Time: 10 min

In your groups do these tasks.
Try not to look ahead, nor in the solution section `r emo::ji("wink")`!
When the time is up, we'll share some ideas and go over what the regex will be.

1. Looking at the `file_path_id` column, 
list what is similar in the user ID between rows and what is different.
1. Discuss and verbally describe (in English, not regex) what text pattern you
might use to extract the user ID.
1. Use this bit of information to think about how you might apply the English
description as a regex. This will probably be very hard, but try anyway.
    - Written a character written as is will find that character, e.g. `user` ***NB: I DON'T UNDERSTAND THE MEANING OF THIS****
    will find only `user`.
    - Use `[]` to find one possible character of the several between the brackets.
    E.g. `[12]` means 1 *or* 2 or `[ab]` means "a" or "b". To find a range of numbers
    or letters, use `-` in between the start and end ranges, e.g. `[1-3]` means
    1 to 3 or `[a-c]` means "a" to "c".
    - Use `?` if the character might be there or not. E.g. `ab?` means "a" and maybe
    "b" follows it or `1[1-2]?` means 1 and maybe 1 or 2 will follow it.
    
Once you've done these tasks, we'll discuss all together and go over what the
regex would be to extract the user ID.

```{r solution-regex, results='hide', solution=TRUE}
"user_[1-9][0-9]?"
```

```{r, results='asis', echo=FALSE}
details_for_instructors("
Make sure to reinforce that while regex is incredibly complicated, there are some
basic things you can do with it that are quite powerful.

More or less, this section and exercise are to introduce the idea and concept of
regex, but not to really teach it since that is well beyond the scope of this
course and this time frame.

Go over the solution. Explanation is that the pattern will find anything that has
`user_` followed by a number from 1 to 9 and maybe followed by another number
from 0 to 9.
")
```

## Using regular expressions to extract text

Now that we've identified a possible regex to use to extract the user ID,
let's test it out on the `user_info_df` data.
Once it works, we will convert it into a function and move it into the
`R/functions.R` file.

Since we will create a new column for the user ID, we will use the `mutate()`
function from the dplyr package. We'll use the `str_extract()` function from the
stringr package to "extract a string" by using the regex `user_[1-9][0-9]?` that
we discussed from the exercise.
In your `doc/lesson.Rmd` file, create a new header called 
`## Using regex for user ID` at the bottom of the document, and create a new
code chunk below that.

`r details_for_instructors("
Walk through writing this code, briefly explain/remind how to use mutate, and
about the stringr function.
")`

```{r extract-user-id, eval=FALSE}
# Note: your file paths and data may look slightly different.
user_info_df %>% 
    mutate(user_id = str_extract(file_path_id, "user_[1-9][0-9]?"), file_path_id)
```

```{r extract-user-id-eval, echo=FALSE}
user_info_df %>% 
    trim_filepath_for_book() %>% 
    mutate(user_id = str_extract(file_path_id, "user_[1-9][0-9]?")) %>% 
    relocate(file_path_id, user_id)
```

Since we don't need the `file_path_id` column anymore, let's drop it using `select()`
and `-`.

```{r drop-file-path-id}
user_info_df %>% 
    mutate(user_id = str_extract(file_path_id, "user_[1-9][0-9]?")) %>% 
    select(-file_path_id)
```

## Exercise: Convert ID extractor code into a function

Time: 15-20 min

We now have code that takes the data that has the `file_path_id` column
and extracts the user ID from it. **First step**: While in the `doc/lesson.Rmd`
file, convert this code into a function, using the same process you've done
previously.

- Call the new function `extract_user_id` and add one argument called
`imported_data`.
    - Remember to output the code into an object and `return()` it at the end
    of the function.
    - Include Roxygen documentation.
- After writing it and testing that the function works, 
move the function into `R/functions.R`.
- Replace the code in the `doc/lesson.Rmd` file with the function name (e.g.
`extract_user_id(user_info_df)`), restart the R session, source everything with
`source()` (`Ctrl-Shift-S`), and run the new function to test that it works.

*Tip*: If you don't know what package a function comes from when you
need to append the package when using `::`, you can find out what the package is
by using the help documentation `?functionname` (can also be done by pressing F1
when the cursor is over the function). The package name is at the very top left
corner, surrounded by `{ }`.

```{r solution-user-id-extract, results='hide', solution=TRUE}
#' Extract user ID from data with file path column.
#'
#' @param imported_data Data with `file_path_id` column.
#'
#' @return A data.frame/tibble.
#'
extract_user_id <- function(imported_data) {
    extracted_id <- imported_data %>% 
        dplyr::mutate(user_id = stringr::str_extract(file_path_id, 
                                                     "user_[0-9][0-9]?")) %>% 
        dplyr::select(-file_path_id)
    return(extracted_id)
}
```

**Second step**: 
Since we want this function to work on all the datasets that we will import, 
we should add it into the `import_multiple_files()` function. 
Go to the `import_multiple_files()` function in `R/functions.R` and use the `%>%` to
add it after using the `map_dfr()` function. The code should look something like:

```{r add-extract-user-to-import}
import_multiple_files <- function(file_pattern, import_function) {
    data_files <- fs::dir_ls(here::here("data-raw/mmash/"),
                             regexp = file_pattern,
                             recurse = TRUE)
    
    combined_data <- purrr::map_dfr(data_files, import_function,
                                    .id = "file_path_id") %>% 
        extract_user_id()
    return(combined_data)
}
```

Re-source the functions with `source()` (`Ctrl-Shift-S`). Then re-run these
pieces of code you wrote during Exercise \@ref(ex-function-import-all-data) to
update them based on the new code in the `import_multiple_files()` function.
Something like this should already be somewhere in your `doc/lesson.Rmd` file.

```{r}
user_info_df <- import_multiple_files("user_info.csv", import_user_info)
saliva_df <- import_multiple_files("saliva.csv", import_saliva)
rr_df <- import_multiple_files("RR.csv", import_rr)
actigraph_df <- import_multiple_files("Actigraph.csv", import_actigraph)
```

And update the `summarised_rr_df` and `summarised_actigraph_df` to use `user_id`
instead of `file_path_id`:

```{r}
summarised_rr_df <- rr_df %>% 
    group_by(user_id, day) %>% 
    summarise(across(ibi_s, list(mean = mean, sd = sd), na.rm = TRUE))

summarised_actigraph_df <- actigraph_df %>% 
    group_by(user_id, day) %>% 
    # These statistics will probably be different for you
    summarise(across(hr, list(mean = mean, sd = sd), na.rm = TRUE))
```

**Final step**: Knit your `doc/lesson.Rmd` document to make sure everything still
runs fine. Then, add and commit all the changed files into the Git history.

## Join datasets together

`r details_for_instructors("
Walk through and describe these images and the different type of joins.
")`

The ability to join datasets together is a fundamental component of data processing
and transformation. In our case, we want to add the datasets together so we 
eventually have preferably one main dataset to work with.

There are many ways to join datasets, the more common ones that are implemented
in the dplyr package are:

- `left_join(x, y)`: Join all rows and columns in `y` that match rows and
columns in `x`. *Columns* that exist in `y` but not `x` are joined to `x`.

    ```{r image-left-join, fig.cap="Left joining in dplyr. Modified from the [RStudio dplyr cheatsheet][dplyr-cheatsheet].", out.width="60%", echo=FALSE}
    knitr::include_graphics(here::here("images/left-join.png"))
    ```
    
- `right_join(x, y)`: The opposite of `left_join()`. Join all rows and columns
in `x` that match rows and columns in `y`. *Columns* that exist in `x` but not `y`
are joined to `y`.

    ```{r image-right-join, fig.cap="Right joining in dplyr. Modified from the [RStudio dplyr cheatsheet][dplyr-cheatsheet].", out.width="60%", echo=FALSE}
    knitr::include_graphics(here::here("images/right-join.png"))
    ```

- `full_join(x, y)`: Join all rows and columns in `y` that match rows and
columns in `x`. Columns *and* **rows** that exist in `y` but not `x` are joined
to `x`. 

    ```{r image-full-join, fig.cap="Full joining in dplyr. Modified from the [RStudio dplyr cheatsheet][dplyr-cheatsheet].", out.width="60%", echo=FALSE}
    knitr::include_graphics(here::here("images/full-join.png"))
    ```

[dplyr-cheatsheet]: https://raw.githubusercontent.com/rstudio/cheatsheets/master/data-transformation.pdf

In our case, we want to use `full_join()`, since we want all the data from both
datasets. This function takes two datasets and lets you indicate which column
to join by using the `by` argument. Here, both datasets have the column `user_id`
so we will join by them.

```{r}
full_join(user_info_df, saliva_df, by = "user_id")
```

`full_join()` is useful if we want to include all values from both datasets,
as long as each participant ("user") had data collected from that dataset.
When the two datasets have rows that don't match, we will get missingness in that
row, but that's ok in this case.

We also eventually have other datasets to join together later on. Since
`full_join()` can only take two datasets at a time, do we then just keep 
using `full_join()` until all the other datasets are combined?
What if we get more data later on? Well, that's where more functional programming
comes in. Again, we have a simple goal: For a set of data frames, join them
all together. Here we use another functional programming concept called `reduce()`.
Like `map()`, which "maps" a function onto a set of items, `reduce()`
applies a function to each item of a vector or list, each time reducing the set
of items down until only one remains: the output. Let's use our simple function
`add_numbers()` from before and add up 1 to 5. Since `add_numbers()` only takes
two numbers, we have to give it two numbers at a time and repeat until we reach
5.

```{r}
# Add from 1 to 5
first <- add_numbers(1, 2)
second <- add_numbers(first, 3)
third <- add_numbers(second, 4)
add_numbers(third, 5)
```

Instead, we can use reduce to do the same thing:

```{r}
reduce(1:5, add_numbers)
```

Figure \@ref(fig:image-reduce) visually shows what is happening within `reduce()`.

```{r image-reduce, fig.cap="A functional that iteratively uses a function on a set of items until only one output remains. Modified from the [RStudio purrr cheatsheet][purrr-cheatsheet].", out.width="75%", echo=FALSE}
knitr::include_graphics(here::here("images/reduce.png"))
```

[purrr-cheatsheet]: https://raw.githubusercontent.com/rstudio/cheatsheets/master/purrr.pdf

If we look at `?reduce`, we see that `reduce()`, like `map()`, takes either a
vector or a list as an input. Since data frames can only be put together as a list 
and not as a vector (a data frame has vectors for columns and so can't be a vector itself), 
we need to combine the datasets together in a `list()` and reduce them with `full_join()`:

```{r}
combined_data <- reduce(list(user_info_df, saliva_df), full_join)
combined_data
```

We now have the data in a form that would make sense to join it with the other
datasets. So lets try it:

```{r}
reduce(list(user_info_df, saliva_df, summarised_rr_df), full_join)
```

Hmm, but wait, we now have four rows of each user, when we should have 
only two, one for each day. By looking at each dataset we joined, we can find that
the `saliva_df` doesn't have a `day` column and instead has a `samples` column. 
We'll need to add a day column in order to join properly with the RR dataset. 
For this, we'll learn about using nested conditionals.

## Cleaning with nested conditionals

**Take 6 min to read through this section, after which we will go over it again**.
There are many ways to clean up this particular problem, but probably the
easiest, most explicit, and programmatically accurate way of doing it would be
with the function `case_when()`. 
This function works by providing it with a series of logical conditions and an
associated output if the condition is true. The general form looks like:

```r
case_when(
    variable1 == condition1 ~ output,
    variable2 == condition2 ~ output,
    # (Optional) Otherwise
    TRUE ~ final_output
)
```

The optional ending is only necessary if you want a certain output if none of your
conditions are met. By default, the final output would be a missing value.
A (silly) example using age might be:

```r
case_when(
    age > 20 ~ "old",
    age <= 20 ~ "young",
    # For final condition
    TRUE ~ "unborn!"
)
```

If instead you want one of the conditions to be `NA`, you need to set the appropriate
`NA` value:

```r
case_when(
    age > 20 ~ "old",
    age <= 20 ~ NA_character_,
    # For final condition
    TRUE ~ "unborn!"
)
```

With dplyr functions like `case_when()`,
it requires you be explicit about the type of output each condition has since 
all the outputs must match (e.g. all character or all numeric).
This prevents you from accidentally mixing e.g. numeric output with character
output. Missing values also have data types:

- `NA_character_` (character)
- `NA_real_` (numeric)
- `NA_integer_` (integer)

Assuming the final output is `NA`, in a pipeline this would look like how you
normally would use `mutate()`:

```{r}
user_info_df %>% 
    mutate(age_category = case_when(
        age > 20 ~ "old",
        age <= 20 ~ "young"
    ))
```

**Ok, please stop reading and we will code together again**.

`r details_for_instructors("Briefly review the content again, to reinforce what they read.")`

By using the `case_when()` function, we can set `"before sleep"` as day 1 and
`"wake up"` as day 2 by creating a new column called `day`. (We will use
`NA_real_` because the other `day` columns are numeric, not integer.)

```{r}
saliva_with_day_df <- saliva_df %>% 
    mutate(day = case_when(
        samples == "before sleep" ~ 1,
        samples == "wake up" ~ 2
    ))
saliva_with_day_df
```

...Now, let's use the `reduce()` with `full_join()` again:

```{r}
reduce(list(user_info_df, saliva_with_day_df, summarised_rr_df), full_join)
```

We now have two rows per participant!

## Wrangling data into final form

Now that we've got several datasets processed and joined, its now time to 
bring it all together and put it into the `data-raw/mmash.R` script so we
can create a final working dataset.

Open up the `data-raw/mmash.R` file to get ready to start cutting and pasting
the code from `doc/lesson.Rmd` that is related to importing and joining the
`user_info.csv`, `saliva.csv`, `RR.csv`, and `Actigraph.csv`.
Before you cut and paste, check that the `data-raw/mmash.R` matches the code
below. 

```{r data-raw-mmash-script, eval=FALSE}
library(here)

# Download
mmash_link <- "https://physionet.org/static/published-projects/mmash/multilevel-monitoring-of-activity-and-sleep-in-healthy-people-1.0.0.zip"
download.file(mmash_link, destfile = here("data-raw/mmash-data.zip"))

# Unzip
unzip(here("data-raw/mmash-data.zip"), 
      exdir = here("data-raw"),
      junkpaths = TRUE)
unzip(here("data-raw/MMASH.zip"),
      exdir = here("data-raw"))

# Remove/tidy up left over files
library(fs)
file_delete(here(c("data-raw/MMASH.zip", 
                   "data-raw/SHA256SUMS.txt",
                   "data-raw/LICENSE.txt")))
file_move(here("data-raw/DataPaper"), here("data-raw/mmash"))
```

First thing to do is comment out the `download.file()`, `unzip()`, `file_delete()`, 
and `file_move()` code, since we don't want
to download and unzip the data every single time we run this script. Next we'll want to
rearrange all the `library()` package calls so they are at the top, and then add
the other packages we'll need: tidyverse and vroom.

```{r}
library(tidyverse)
library(vroom)
library(fs)
library(here)
```

Next, we need to let this script know where to get our functions from. So right
below the `library()` functions, add:

```r
source(here("R/functions.R"))
```

This is the code that runs when you use `Ctrl-Shift-S` (`source()`).

Now, find the code for `import_multiple_files()`, summarized RR and Actigraph
data, saliva data with the day column, and the `reduce()` code in your
`doc/lesson.Rmd` file and cut and paste it at the bottom of the
`data-raw/mmash.R` script. This is what the script should look like now:

```{r data-raw-mmash-2, eval=FALSE}
library(tidyverse)
library(vroom)
library(fs)
library(here)
source(here("R/functions.R"))

# Download
mmash_link <- "https://physionet.org/static/published-projects/mmash/multilevel-monitoring-of-activity-and-sleep-in-healthy-people-1.0.0.zip"
# download.file(mmash_link, destfile = here("data-raw/mmash-data.zip"))

# Unzip
# unzip(here("data-raw/mmash-data.zip"), 
#       exdir = here("data-raw"),
#       junkpaths = TRUE)
# unzip(here("data-raw/MMASH.zip"),
#       exdir = here("data-raw"))

# Remove/tidy up left over files
# file_delete(here(c("data-raw/MMASH.zip", 
#                    "data-raw/SHA256SUMS.txt",
#                    "data-raw/LICENSE.txt")))
# file_move(here("data-raw/DataPaper"), here("data-raw/mmash"))

user_info_df <- import_multiple_files("user_info.csv", import_user_info)
saliva_df <- import_multiple_files("saliva.csv", import_saliva)
rr_df <- import_multiple_files("RR.csv", import_rr)
actigraph_df <- import_multiple_files("Actigraph.csv", import_actigraph)

summarised_rr_df <- rr_df %>% 
    group_by(user_id, day) %>% 
    summarise(across(ibi_s, list(mean = mean, sd = sd), na.rm = TRUE))

# Your Actigraph code will be probably be different
summarised_actigraph_df <- actigraph_df %>% 
    group_by(user_id, day) %>% 
    summarise(across(hr, list(mean = mean, sd = sd)))

saliva_with_day_df <- saliva_df %>% 
    mutate(day = case_when(
        samples == "before sleep" ~ 1,
        samples == "wake up" ~ 2,
        TRUE ~ NA_real_
    ))

mmash <- reduce(
    list(
        user_info_df,
        saliva_with_day_df,
        summarised_rr_df,
        summarised_actigraph_df
    ),
    full_join
)
```

Lastly, we have to save this final dataset into the `data/` folder. We'll use the
function `usethis::use_data()` to create the folder and save the data as an `.rda`
file. We'll add this code to the very bottom of the script:

```{r, eval=FALSE}
usethis::use_data(mmash, overwrite = TRUE)
```

We're adding `overwrite = TRUE` so every time we re-run this script, the dataset
will be saved. If the final dataset is going to be really large, we could save
it as a `.csv` file with:

```{r, eval=FALSE}
vroom_write(mmash, here("data/mmash.csv"))
```

And later load it in with `vroom()` (since it is so fast).
Alright, we're finished creating this dataset! Let's generate it
by:

- Restarting the R session with `Ctrl-Shift-F10` ("Session -> Restart R").
- Sourcing the `data-raw/mmash.R` script with `Ctrl-Shift-S` ("Code -> Source" or
the "Source" button in the top right corner of the script pane).

We now have a final dataset to start working on! The main way to load data is
with `load(here::here("data/mmash.rda"))`. Lastly, add and commit all the changes,
including adding the final `mmash.rda` data file, to the Git history. Make sure
the `doc/lesson.Rmd` file is empty (except for the YAML header) and ready for
the next session.

## Exercise: What other cleaned data might you create?

Time: 15 min

In order to meaningfully join all the datasets together, we had to calculate
summary measures for the `RR.csv` and `Actigraph.csv` data. However,
we also lose a lot of information and potentially interesting analyses from it.
With your group, discuss some potential ways that you might join the datasets
to retain that information and what questions you might be interested in with
that data. We'll discuss all the ideas afterwards.

## Exercise: Import and process the activity data

Time: 25 minutes

**This is an optional exercise if there is extra time (or to do during the group work session)**.
We have a few other datasets that we could join together, but would likely require
more processing in order to appropriately join with the other datasets.
Complete these tasks in the `doc/lesson.Rmd` file:

1. Create a new header called `## Exercise: Importing activity data`
1. Create a new code chunk below this new header.
1. Starting the workflow from the beginning (e.g. with the `spec()` process),
write code that imports the `Activity.csv` data into R. 
1. Convert this code into a new function using the workflow you've used from
this course:
    - Call the new function `import_activity`.
    - Include one argument called `file_path`
    - Test that it works.
    - Add Roxygen documentation and explicit package links (`::`) with the
    functions.
    - Move the newly created function into the `R/functions.R`.
    - Use the new function in `doc/lesson.Rmd` and use `source()`
    (`Ctrl-Shift-S`) to run it.
1. Import all the `user_` datasets with `import_multiple_files()` and the
`import_activity()` function.
1. Pipe the results into `mutate()` and create a new column called
`activity_seconds` that is based on subtracting `end` and `start`.
    - Use `?mutate` and check the examples if you don't recall how to use this ****INSERT LINK?****
    function.
1. Add and commit your changes to the Git history.

```{r solution-import-activity, results='hide', solution=TRUE, eval=FALSE}
import_activity <- function(file_path) {
    activity_data <- vroom::vroom(
        file_path,
        col_select = -1,
        col_types = vroom::cols(
            Activity = vroom::col_double(),
            Start = vroom::col_time(format = ""),
            End = vroom::col_time(format = ""),
            Day = vroom::col_double()
        ),
        .name_repair = snakecase::to_snake_case
    ) 
    return(activity_data)
}

activity_df <- import_multiple_files("Activity.csv", import_activity)

activity_df %>% 
    mutate(activity_duration = end - start)
```


Look into the [Data Description][mmash-site] and find out what each column
represents and what the numbers mean in the column `activity`. Then brainstorm
with your group:

1. The disadvantage of using numbers instead of text to describe categorical
data like in the `activity` column.
1. Ways you could include this information into the dataset.
1. How you could meaningfully join this dataset with the other datasets.

[mmash-site]: https://physionet.org/content/mmash/1.0.0/
